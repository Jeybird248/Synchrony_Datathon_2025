{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0n1IFY3ZfffG"
      },
      "outputs": [],
      "source": [
        "# PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load Dataset\n",
        "merged_df = pd.read_csv('cleaned_4.csv')\n",
        "\n",
        "# Preprocess\n",
        "non_acc_cols = ['current_account_nbr', 'client_id', 'open_date', 'card_activation_date', 'open_date_naive']\n",
        "acc_cols = merged_df.drop(columns=non_acc_cols, errors='ignore')\n",
        "\n",
        "numeric_cols = acc_cols.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_cols = acc_cols.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "threshold = 0.8  # keep only columns with >= 80% non-null\n",
        "valid_cols = acc_cols[numeric_cols].dropna(thresh=threshold * len(acc_cols)).columns\n",
        "\n",
        "X = acc_cols[valid_cols].replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Load Dataset\n",
        "merged_df = pd.read_csv('cleaned_4.csv')\n",
        "\n",
        "# Preprocess\n",
        "non_acc_cols = ['current_account_nbr', 'client_id', 'open_date', 'card_activation_date', 'open_date_naive']\n",
        "acc_cols = merged_df.drop(columns=non_acc_cols, errors='ignore')\n",
        "\n",
        "numeric_cols = acc_cols.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_cols = acc_cols.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "threshold = 0.8  # keep only columns with >= 80% non-null\n",
        "valid_cols = acc_cols[numeric_cols].dropna(thresh=threshold * len(acc_cols)).columns\n",
        "\n",
        "X = acc_cols[valid_cols].replace([np.inf, -np.inf], np.nan)\n",
        "X = X.fillna(X.mean())\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# K-Means Clustering\n",
        "for n_clusters in range(3, 8):\n",
        "    print(f\"\\n K-Means with {n_clusters} Clusters \")\n",
        "    k_means = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = k_means.fit_predict(X_scaled)\n",
        "\n",
        "    pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "    pca_df['Cluster'] = labels\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for i in range(n_clusters):\n",
        "        plt.scatter(\n",
        "            pca_df[pca_df['Cluster'] == i]['PC1'],\n",
        "            pca_df[pca_df['Cluster'] == i]['PC2'],\n",
        "            label=f'Cluster {i}',\n",
        "            alpha=0.6\n",
        "        )\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.title(f'Customer Segmentation: PCA (K-Means, k={n_clusters})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# k_means = KMeans(n_clusters=4, random_state=42)\n",
        "# labels = kmeans.fit_predict(X_scaled)\n",
        "# X['cluster'] = labels\n",
        "\n",
        "feature_names = X.columns.tolist()\n",
        "components = pca.components_\n",
        "\n",
        "top_k = 10  # You can change this to whatever number you want\n",
        "\n",
        "print(f\"\\nTop {top_k} Contributing Features per Principal Component:\\n\")\n",
        "for i, comp in enumerate(components):\n",
        "    print(f\"PC{i+1}:\")\n",
        "    component_weights = zip(feature_names, comp)\n",
        "    sorted_weights = sorted(component_weights, key=lambda x: abs(x[1]), reverse=True)\n",
        "    for feature, weight in sorted_weights[:top_k]:\n",
        "        print(f\"  {feature:<35} | Weight: {weight:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# # PCA visualize\n",
        "# pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "# pca_df['Cluster'] = labels\n",
        "# plt.figure(figsize=(8,6))\n",
        "# for i in range(4):\n",
        "#     plt.scatter(pca_df[pca_df['Cluster'] == i]['PC1'], pca_df[pca_df['Cluster'] == i]['PC2'], label=f'Cluster {i}', alpha=0.6)\n",
        "\n",
        "# plt.xlabel('PC1')\n",
        "# plt.ylabel('PC2')\n",
        "# plt.title('Customer Segmentation: PCA Visualization of K-Means Clusters')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()\n",
        "feature_names = X.columns.tolist()\n",
        "components = pca.components_\n",
        "\n",
        "top_k = 10  # You can change this to whatever number you want\n",
        "\n",
        "print(f\"\\nTop {top_k} Contributing Features per Principal Component:\\n\")\n",
        "for i, comp in enumerate(components):\n",
        "    print(f\"PC{i+1}:\")\n",
        "    component_weights = zip(feature_names, comp)\n",
        "    sorted_weights = sorted(component_weights, key=lambda x: abs(x[1]), reverse=True)\n",
        "    for feature, weight in sorted_weights[:top_k]:\n",
        "        print(f\"  {feature:<35} | Weight: {weight:.4f}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# K-Means Clustering\n",
        "for n_clusters in range(3, 8):\n",
        "    print(f\"\\n K-Means with {n_clusters} Clusters \")\n",
        "    k_means = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = k_means.fit_predict(X_scaled)\n",
        "\n",
        "    pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "    pca_df['Cluster'] = labels\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "    for i in range(n_clusters):\n",
        "        plt.scatter(\n",
        "            pca_df[pca_df['Cluster'] == i]['PC1'],\n",
        "            pca_df[pca_df['Cluster'] == i]['PC2'],\n",
        "            label=f'Cluster {i}',\n",
        "            alpha=0.6\n",
        "        )\n",
        "    plt.xlabel('PC1')\n",
        "    plt.ylabel('PC2')\n",
        "    plt.title(f'Customer Segmentation: PCA (K-Means, k={n_clusters})')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# k_means = KMeans(n_clusters=4, random_state=42)\n",
        "# labels = kmeans.fit_predict(X_scaled)\n",
        "# X['cluster'] = labels\n",
        "\n",
        "\n",
        "\n",
        "# # PCA visualize\n",
        "# pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "# pca_df['Cluster'] = labels\n",
        "# plt.figure(figsize=(8,6))\n",
        "# for i in range(4):\n",
        "#     plt.scatter(pca_df[pca_df['Cluster'] == i]['PC1'], pca_df[pca_df['Cluster'] == i]['PC2'], label=f'Cluster {i}', alpha=0.6)\n",
        "\n",
        "# plt.xlabel('PC1')\n",
        "# plt.ylabel('PC2')\n",
        "# plt.title('Customer Segmentation: PCA Visualization of K-Means Clusters')\n",
        "# plt.legend()\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <Version 2>\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load cleaned data\n",
        "df = pd.read_csv(\"cleaned_4.csv\")\n",
        "\n",
        "# Define features\n",
        "spending_features = [\n",
        "    'avg_monthly_sales_last_6_months',\n",
        "    '2024Q3_SALE_total_spending',\n",
        "    '2024Q4_SALE_total_spending',\n",
        "    '2025Q1_SALE_total_spending',\n",
        "    'sales_trend',\n",
        "    'sales_volatility'\n",
        "]\n",
        "\n",
        "risk_features = [\n",
        "    \"utilization_rate\", \"open_to_buy\", \"current_balance\", \"max_days_delinquent\",\n",
        "    \"current_cycles_delinquent\", \"balance_to_credit_ratio\", \"cash_balance_to_cash_line_ratio\",\n",
        "    \"credit_grade_numeric\", \"months_since_active\", \"months_since_credit_limit_change\",\n",
        "    \"utilization_stability\", \"financial_stress_indicator\", \"months_on_books\",\n",
        "    \"nsf_count_last_12_months\", \"behavior_score_change\", \"behavior_score_percent_change\",\n",
        "    \"high_delinquency_flag\", \"rapid_balance_change_flag\", \"fraud_risk_score\",\n",
        "    \"sales_volatility\", \"spending_to_balance_ratio\", \"external_credit_score\",\n",
        "    \"weighted_utilization\", \"credit_limit_utilization_trend\", \"income_stability_score\",\n",
        "    \"cash_balance\", \"cash_balance_pct_credit_line\", \"cash_buffer_ratio\"\n",
        "]\n",
        "\n",
        "# Clean and scale\n",
        "# df_clean = df[spending_features + risk_features].replace([np.inf, -np.inf], np.nan).dropna()\n",
        "df_clean = df[spending_features + risk_features].replace([np.inf, -np.inf], np.nan)\n",
        "df_clean = df_clean.fillna(df_clean.mean())\n",
        "scaler = StandardScaler()\n",
        "spending_scaled = scaler.fit_transform(df_clean[spending_features])\n",
        "risk_scaled = scaler.fit_transform(df_clean[risk_features])\n",
        "\n",
        "# Get composite scores (take average of scaled values)\n",
        "df_clean['spending_score'] = spending_scaled.mean(axis=1)\n",
        "df_clean['risk_score'] = risk_scaled.mean(axis=1)\n",
        "\n",
        "# K-Means Clustering on 2D (risk_score, spending_score)\n",
        "X_cluster = df_clean[['risk_score', 'spending_score']]\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "df_clean['cluster'] = kmeans.fit_predict(X_cluster)\n",
        "centroids = kmeans.cluster_centers_\n",
        "\n",
        "print(pd.DataFrame(centroids, columns=['risk_score', 'spending_score']))\n",
        "\n",
        "\n",
        "def quadrant_label(row):\n",
        "    if row['spending_score'] > 0 and row['risk_score'] < 0:\n",
        "        return \"High Spending, Low Risk\"\n",
        "    elif row['spending_score'] > 0 and row['risk_score'] > 0:\n",
        "        return \"High Spending, High Risk\"\n",
        "    elif row['spending_score'] < 0 and row['risk_score'] < 0:\n",
        "        return \"Low Spending, Low Risk\"\n",
        "    else:\n",
        "        return \"Low Spending, High Risk\"\n",
        "\n",
        "df_clean['quadrant_label'] = df_clean.apply(quadrant_label, axis=1)\n",
        "print(df_clean['quadrant_label'].value_counts())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(\n",
        "    data=df_clean,\n",
        "    x='risk_score',\n",
        "    y='spending_score',\n",
        "    hue='quadrant_label',\n",
        "    palette='viridis',\n",
        "    s=50,\n",
        "    alpha=0.7\n",
        ")\n",
        "plt.scatter(\n",
        "    centroids[:, 0], centroids[:, 1],\n",
        "    c='red', s=200, marker='X', label='Centroids'\n",
        ")\n",
        "plt.axhline(0, color='gray', linestyle='—', linewidth=0.8)\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
        "plt.title(\"Customer Segments: Risk vs Spending (K-Means)\")\n",
        "plt.xlabel(\"Composite Risk Score\")\n",
        "plt.ylabel(\"Composite Spending Score\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gnI0KNLKfi_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Regression for Selected Feature\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load both datasets\n",
        "df_main = pd.read_csv(\"cleaned_4.csv\")\n",
        "df_creditline = pd.read_csv(\"data/rams_batch_cur_20250325.csv\")\n",
        "\n",
        "df_merged = pd.merge(\n",
        "    df_main,\n",
        "    df_creditline[['cu_account_nbr', 'cu_crd_line']],\n",
        "    left_on='current_account_nbr',\n",
        "    right_on='cu_account_nbr',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Drop rows where target is missing\n",
        "df_merged = df_merged.dropna(subset=['cu_crd_line'])\n",
        "\n",
        "# Define features\n",
        "spending_features = [\n",
        "    'avg_monthly_sales_last_6_months',\n",
        "    '2024Q3_SALE_total_spending',\n",
        "    '2024Q4_SALE_total_spending',\n",
        "    '2025Q1_SALE_total_spending',\n",
        "    'sales_trend',\n",
        "    'sales_volatility'\n",
        "]\n",
        "\n",
        "risk_features = [\n",
        "    'utilization_rate', 'open_to_buy', 'current_balance', 'max_days_delinquent',\n",
        "    'current_cycles_delinquent', 'balance_to_credit_ratio', 'cash_balance_to_cash_line_ratio',\n",
        "    'credit_grade_numeric', 'months_since_active', 'months_since_credit_limit_change',\n",
        "    'utilization_stability', 'financial_stress_indicator', 'months_on_books',\n",
        "    'total_sales_last_6_months', 'avg_monthly_sales_last_6_months', 'sales_trend',\n",
        "    'nsf_count_last_12_months', 'behavior_score_change', 'behavior_score_percent_change',\n",
        "    'high_delinquency_flag', 'rapid_balance_change_flag', 'fraud_risk_score',\n",
        "    'sales_volatility', 'spending_to_balance_ratio', 'external_credit_score',\n",
        "    'weighted_utilization', 'credit_limit_utilization_trend', 'income_stability_score',\n",
        "    'cash_balance', 'cash_balance_pct_credit_line', 'cash_buffer_ratio'\n",
        "]\n",
        "\n",
        "features = list(set(spending_features + risk_features))\n",
        "\n",
        "# Drop rows with NaNs in features\n",
        "df_model = df_merged[features + ['cu_crd_line']].replace([np.inf, -np.inf], pd.NA).dropna()\n",
        "\n",
        "# Split\n",
        "X = df_model[features]\n",
        "y = df_model['cu_crd_line']\n",
        "\n",
        "# Scale\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.2f}\")\n",
        "print(f\"MAE (Mean Absolute Error):     {mae:.2f}\")\n",
        "print(f\"MAPE (Mean Absolute % Error):  {mape:.2f}%\")"
      ],
      "metadata": {
        "id": "Cmjv2qizflD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Regerssion using all data info\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "# Load datasets\n",
        "merged_df = pd.read_csv('cleaned_4.csv')\n",
        "rams_df = pd.read_csv('data/rams_batch_cur_20250325.csv')\n",
        "\n",
        "# print(\"cleaned_4.csv columns:\\n\", pd.read_csv(\"cleaned_4.csv\").columns.tolist())\n",
        "# print(\"\\nrams_batch_cur_20250325.csv columns:\\n\", pd.read_csv(\"data/rams_batch_cur_20250325.csv\").columns.tolist())\n",
        "\n",
        "\n",
        "# df_merged = pd.merge(merged_df, rams_df[['cu_account_nbr', 'cu_crd_line']], on='cu_account_nbr', how='inner')\n",
        "df_merged = pd.merge(\n",
        "    merged_df,\n",
        "    rams_df[['cu_account_nbr', 'cu_crd_line']],\n",
        "    left_on='current_account_nbr',\n",
        "    right_on='cu_account_nbr',\n",
        "    how='inner'\n",
        ")\n",
        "df_merged = df_merged.dropna(subset=['cu_crd_line'])\n",
        "\n",
        "drop_cols = ['current_account_nbr', 'client_id', 'open_date', 'card_activation_date', 'open_date_naive']\n",
        "df_features = df_merged.drop(columns=drop_cols, errors='ignore')\n",
        "\n",
        "\n",
        "df_numeric = df_features.select_dtypes(include=['float64', 'int64'])\n",
        "# df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)\n",
        "df_numeric = df_numeric.fillna(df_numeric.mean())\n",
        "\n",
        "X = df_numeric.drop(columns=['cu_crd_line'], errors='ignore')\n",
        "y = df_numeric['cu_crd_line']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "# mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "mape = np.mean(np.abs((y_test - y_pred)[y_test != 0] / y_test[y_test != 0])) * 100\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Using all numeric features:\")\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "print(f\"RMSE (Root Mean Squared Error): {rmse:.2f}\")\n",
        "print(f\"MAE (Mean Absolute Error):     {mae:.2f}\")\n",
        "print(f\"MAPE (Mean Absolute % Error):  {mape:.2f}%\")"
      ],
      "metadata": {
        "id": "_GdpuSQzfpJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XlVSaWbMfpDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}